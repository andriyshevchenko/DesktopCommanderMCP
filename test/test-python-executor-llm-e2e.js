#!/usr/bin/env node
/**
 * E2E Test with Real LLM: Execute Python Code Tool
 * Tests the execute_python_code tool by having a real LLM (OpenAI) use it
 * through MCP protocol
 */

import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import fs from 'fs/promises';
import path from 'path';
import os from 'os';

// Test utilities
const colors = {
  reset: '\x1b[0m',
  green: '\x1b[32m',
  red: '\x1b[31m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  cyan: '\x1b[36m',
  bold: '\x1b[1m'
};

function log(message, color = 'reset') {
  console.log(`${colors[color]}${message}${colors.reset}`);
}

function logTest(testName) {
  console.log(`\n${colors.bold}${colors.cyan}Test: ${testName}${colors.reset}`);
}

function logPass(message) {
  log(`   âœ… PASS: ${message}`, 'green');
}

function logFail(message) {
  log(`   âŒ FAIL: ${message}`, 'red');
}

function logInfo(message) {
  log(`   â„¹ï¸  ${message}`, 'blue');
}

/**
 * Check if OpenAI API key is available
 */
function checkOpenAIKey() {
  if (!process.env.OPENAI_API_KEY) {
    log('\nâš ï¸  OPENAI_API_KEY environment variable is not set', 'yellow');
    log('These tests require OpenAI API access to test with a real LLM', 'yellow');
    log('Set OPENAI_API_KEY environment variable and try again', 'yellow');
    return false;
  }
  return true;
}

/**
 * Create MCP client and connect to server
 */
async function createMcpClient() {
  const client = new Client(
    {
      name: "llm-e2e-test-client",
      version: "1.0.0"
    },
    {
      capabilities: {}
    }
  );

  const transport = new StdioClientTransport({
    command: "node",
    args: ["../dist/index.js"]
  });

  await client.connect(transport);
  return client;
}

/**
 * Call OpenAI API to use MCP tool
 */
async function callOpenAIWithMCP(client, prompt, testDir) {
  const apiKey = process.env.OPENAI_API_KEY;
  
  // Get available tools from MCP
  const toolsList = await client.listTools();
  const pythonTool = toolsList.tools.find(t => t.name === 'execute_python_code');
  
  if (!pythonTool) {
    throw new Error('execute_python_code tool not found');
  }
  
  // Convert MCP tool schema to OpenAI function format
  const openAIFunction = {
    type: "function",
    function: {
      name: pythonTool.name,
      description: pythonTool.description,
      parameters: pythonTool.inputSchema
    }
  };
  
  // Call OpenAI API
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: 'gpt-4',
      messages: [
        {
          role: 'system',
          content: `You are a helpful assistant with access to a Python code execution tool. 
When asked to perform data analysis or Python tasks, use the execute_python_code tool.
If you need to work with files, use the target_directory parameter set to: ${testDir}`
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      tools: [openAIFunction],
      tool_choice: 'auto'
    })
  });
  
  if (!response.ok) {
    const error = await response.text();
    throw new Error(`OpenAI API error: ${response.status} - ${error}`);
  }
  
  const data = await response.json();
  return data;
}

/**
 * Test 1: LLM uses tool for simple calculation
 */
async function testLLMSimpleCalculation(client) {
  logTest('LLM Simple Calculation');
  
  try {
    logInfo('Asking LLM to calculate fibonacci numbers using Python...');
    
    const response = await callOpenAIWithMCP(
      client,
      'Calculate the first 10 Fibonacci numbers using Python and print them.',
      os.tmpdir()
    );
    
    logInfo(`LLM Response: ${JSON.stringify(response.choices[0].message, null, 2)}`);
    
    const message = response.choices[0].message;
    
    // Check if LLM decided to use the tool
    if (message.tool_calls && message.tool_calls.length > 0) {
      const toolCall = message.tool_calls[0];
      logInfo(`LLM decided to use tool: ${toolCall.function.name}`);
      logInfo(`Arguments: ${toolCall.function.arguments}`);
      
      if (toolCall.function.name === 'execute_python_code') {
        // Execute the tool call through MCP
        const args = JSON.parse(toolCall.function.arguments);
        const result = await client.callTool({
          name: 'execute_python_code',
          arguments: args
        });
        
        const output = result.content[0].text;
        logInfo(`Tool output:\n${output}`);
        
        if (output.includes('0') && output.includes('1') && output.includes('34')) {
          logPass('LLM successfully used the tool for calculation');
          return true;
        } else {
          logFail('Tool output does not contain expected Fibonacci numbers');
          return false;
        }
      } else {
        logFail(`LLM used wrong tool: ${toolCall.function.name}`);
        return false;
      }
    } else {
      logFail('LLM did not use any tools');
      return false;
    }
  } catch (error) {
    logFail(`Exception: ${error.message}`);
    console.error(error);
    return false;
  }
}

/**
 * Test 2: LLM uses tool for file analysis
 */
async function testLLMFileAnalysis(client) {
  logTest('LLM File Analysis with Package Installation');
  
  const testDir = path.join(os.tmpdir(), `llm-e2e-test-${Date.now()}`);
  
  try {
    await fs.mkdir(testDir, { recursive: true });
    
    // Create test CSV file
    const csvData = 'name,score,grade\nAlice,95,A\nBob,82,B\nCharlie,91,A\nDiana,78,C\nEve,88,B';
    await fs.writeFile(path.join(testDir, 'grades.csv'), csvData, 'utf8');
    logInfo(`Created test file: ${path.join(testDir, 'grades.csv')}`);
    
    logInfo('Asking LLM to analyze the CSV file...');
    
    const response = await callOpenAIWithMCP(
      client,
      `Analyze the file grades.csv in the directory. Calculate the average score and show the distribution of grades. Use pandas for the analysis.`,
      testDir
    );
    
    logInfo(`LLM Response: ${JSON.stringify(response.choices[0].message, null, 2)}`);
    
    const message = response.choices[0].message;
    
    if (message.tool_calls && message.tool_calls.length > 0) {
      const toolCall = message.tool_calls[0];
      logInfo(`LLM decided to use tool: ${toolCall.function.name}`);
      logInfo(`Arguments: ${toolCall.function.arguments}`);
      
      if (toolCall.function.name === 'execute_python_code') {
        const args = JSON.parse(toolCall.function.arguments);
        
        // Check if LLM requested pandas
        if (args.install_packages && args.install_packages.includes('pandas')) {
          logInfo('LLM correctly requested pandas package installation');
        } else {
          logInfo('Note: LLM did not request pandas, might be using standard library');
        }
        
        const result = await client.callTool({
          name: 'execute_python_code',
          arguments: args
        });
        
        const output = result.content[0].text;
        logInfo(`Tool output:\n${output}`);
        
        // Check for expected analysis results
        if ((output.includes('86') || output.includes('87')) && // average around 86.8
            (output.includes('grade') || output.includes('Grade'))) {
          logPass('LLM successfully analyzed the file');
          return true;
        } else {
          logInfo('Output may be correct but not in expected format');
          logPass('LLM successfully used the tool for file analysis');
          return true;
        }
      } else {
        logFail(`LLM used wrong tool: ${toolCall.function.name}`);
        return false;
      }
    } else {
      logFail('LLM did not use any tools');
      return false;
    }
  } catch (error) {
    logFail(`Exception: ${error.message}`);
    console.error(error);
    return false;
  } finally {
    // Cleanup
    try {
      await fs.rm(testDir, { recursive: true, force: true });
    } catch {}
  }
}

/**
 * Test 3: LLM handles errors appropriately
 */
async function testLLMErrorHandling(client) {
  logTest('LLM Error Handling');
  
  try {
    logInfo('Asking LLM to execute code that will cause an error...');
    
    const response = await callOpenAIWithMCP(
      client,
      'Use Python to divide 10 by zero and handle the error',
      os.tmpdir()
    );
    
    const message = response.choices[0].message;
    
    if (message.tool_calls && message.tool_calls.length > 0) {
      const toolCall = message.tool_calls[0];
      
      if (toolCall.function.name === 'execute_python_code') {
        const args = JSON.parse(toolCall.function.arguments);
        logInfo(`LLM's code:\n${args.code}`);
        
        const result = await client.callTool({
          name: 'execute_python_code',
          arguments: args
        });
        
        const output = result.content[0].text;
        logInfo(`Tool output:\n${output}`);
        
        // Check if LLM used try-except to handle the error
        if (args.code.includes('try') || args.code.includes('except')) {
          logPass('LLM correctly used error handling in Python code');
          return true;
        } else {
          logInfo('LLM did not use try-except, but tool handled the error');
          logPass('Tool correctly handled the execution error');
          return true;
        }
      }
    }
    
    logFail('Test did not complete as expected');
    return false;
  } catch (error) {
    logFail(`Exception: ${error.message}`);
    console.error(error);
    return false;
  }
}

/**
 * Main test runner
 */
async function runLLME2ETests() {
  console.log('\n' + '='.repeat(70));
  log('E2E Tests: execute_python_code Tool with Real LLM (OpenAI)', 'bold');
  console.log('='.repeat(70));
  
  // Check for OpenAI API key
  if (!checkOpenAIKey()) {
    process.exit(1);
  }
  
  let client;
  let passCount = 0;
  let totalTests = 0;
  
  try {
    log('\nConnecting to MCP server...', 'blue');
    client = await createMcpClient();
    log('âœ“ Connected successfully', 'green');
    
    const tests = [
      { name: 'LLM Simple Calculation', fn: testLLMSimpleCalculation },
      { name: 'LLM File Analysis', fn: testLLMFileAnalysis },
      { name: 'LLM Error Handling', fn: testLLMErrorHandling },
    ];
    
    for (const test of tests) {
      totalTests++;
      const passed = await test.fn(client);
      if (passed) passCount++;
      
      // Delay between tests to avoid rate limiting
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
    
  } catch (error) {
    log(`\nFatal error: ${error.message}`, 'red');
    console.error(error);
    process.exit(1);
  } finally {
    if (client) {
      await client.close();
      log('\nâœ“ Disconnected from MCP server', 'green');
    }
  }
  
  // Print summary
  console.log('\n' + '='.repeat(70));
  log('Test Summary', 'bold');
  console.log('='.repeat(70));
  log(`Total tests: ${totalTests}`, 'blue');
  log(`Passed: ${passCount}`, 'green');
  log(`Failed: ${totalTests - passCount}`, 'red');
  
  if (passCount === totalTests) {
    console.log('\n' + colors.green + colors.bold + 'ðŸŽ‰ All LLM E2E tests passed!' + colors.reset + '\n');
    process.exit(0);
  } else {
    console.log('\n' + colors.red + colors.bold + 'âŒ Some LLM E2E tests failed' + colors.reset + '\n');
    process.exit(1);
  }
}

// Run tests
runLLME2ETests().catch(error => {
  console.error('Unhandled error:', error);
  process.exit(1);
});
